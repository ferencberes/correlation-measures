{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datawand.parametrization import ParamHelper\n",
    "ph = ParamHelper(\"../../pipelines/CorrelationComputation.json\",sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset and experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_folder = ph.get(\"experiment_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_id = ph.get(\"dataset_id\")\n",
    "measure_id = ph.get(\"measure_id\")\n",
    "data_path = '/mnt/idms/fberes/network/DATA/temporal_centralities/centrality_output_for_datasets/%s/centrality_scores/' % dataset_id\n",
    "correlation_and_stat_path = '%s/correlations/' % experiment_folder\n",
    "result_folder = '%s/results/corr_and_stats/' % experiment_folder\n",
    "corr_types = ph.get(\"corr_types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect statistics about the number of active users and overlaps for consecutive days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(correlation_and_stat_path + \"/%s_%s.stats\" % (dataset_id, measure_id), sep=\" \")\n",
    "result_df['day'] = range(1,len(result_df)+1)\n",
    "result_df = result_df[[\"day\",\"curr_day_count\",\"prev_day_count\",\"overlap_count\"]]\n",
    "print result_df.index\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization with total vertex count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_score_map_indices(input_prefix, day, measure):\n",
    "    \"\"\"The centrality maps were pre-sorted in decreasing order!!!\"\"\"\n",
    "    scores = pd.read_csv(input_prefix + '/%s_scores_%i.txt_s' % (measure,day), sep=\" \", names=[\"id\",\"score\"])\n",
    "    scores = scores.set_index(\"id\")\n",
    "    return scores.index\n",
    "\n",
    "def get_total_vertex_count(input_prefix, days, measure):\n",
    "    all_indices = set(load_score_map_indices(input_prefix, days[0], measure))\n",
    "    for i in xrange(1,len(days)):\n",
    "        #print i, len(all_indices)\n",
    "        curr_indices = set(load_score_map_indices(input_prefix, days[i], measure))\n",
    "        all_indices = all_indices.union(curr_indices)\n",
    "    return len(all_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_vertex_count = get_total_vertex_count(data_path, range(len(result_df)+1), measure_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print total_vertex_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(result_folder + '/%s_total_vertex_count.txt' % dataset_id, 'w') as f:\n",
    "    f.write(str(total_vertex_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df['curr_day_frac'] = result_df['curr_day_count'] / total_vertex_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df['prev_day_frac'] = result_df['prev_day_count'] / total_vertex_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df['overlap_frac'] = result_df['overlap_count'] / total_vertex_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect weighted kendall results\n",
    "\n",
    "   * calculating weighted kendall is time consuming\n",
    "   * that is why we compute weighted kendall for consecutive days in parallel\n",
    "   * the weighted kendall scores for different say pairs are collected here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_kendall_tmp_folder = \"%s/correlations_tmp\" % experiment_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if \"w_kendall\" in corr_types:\n",
    "    with open(\"%s/correlations/%s_%s.w_kendall\" % (experiment_folder,dataset_id,measure_id),\"w\") as f_out:\n",
    "        for idx in xrange(len(result_df)):\n",
    "            f_partial = \"%s/%s_%s_%i.w_kendall\" % (w_kendall_tmp_folder,dataset_id,measure_id,idx)\n",
    "            if os.path.exists(f_partial):\n",
    "                with open(f_partial) as f:\n",
    "                    for line in f:\n",
    "                        f_out.write(\"%i %f\\n\" % (idx,float(line)))\n",
    "            else:\n",
    "                f_out.write(\"%i %f\\n\" % (idx,-2))\n",
    "        print \"Partial w_kendall results were collected!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect correlation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for corr_type in corr_types:\n",
    "    corr_df = pd.read_csv(correlation_and_stat_path + \"/%s_%s.%s\" % (dataset_id, measure_id,corr_type), sep=\" \", names=[\"idx\",corr_type])[corr_type]\n",
    "    result_df = result_df.join(corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(result_folder + '/%s_%s.csv' % (dataset_id, measure_id), sep=\" \", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dm-env]",
   "language": "python",
   "name": "conda-env-dm-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}